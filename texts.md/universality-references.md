References
Arditi et al. (2024)
Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, and Neel Nanda.Refusal in language models is mediated by a single direction, 2024.
Bansal et al. (2021)
Yamini Bansal, Preetum Nakkiran, and Boaz Barak.Revisiting model stitching to compare neural representations.NIPS ’21, Red Hook, NY, USA, 2021. Curran Associates Inc.ISBN 9781713845393.
Barez et al. (2023)
Fazl Barez, Hosien Hasanbieg, and Alesandro Abbate.System iii: Learning with domain knowledge for safety constraints, 2023.
Bereska and Gavves (2024)
Leonard Bereska and Efstratios Gavves.Mechanistic interpretability for ai safety – a review, 2024.URL https://arxiv.org/abs/2404.14082.
Biderman et al. (2023)
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal.Pythia: A suite for analyzing large language models across training and scaling, 2023.URL https://arxiv.org/abs/2304.01373.
Bloom and Chanin (2024)
Joseph Bloom and David Chanin.Saelens.https://github.com/jbloomAus/SAELens, 2024.
Bricken et al. (2023)
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah.Towards monosemanticity: Decomposing language models with dictionary learning.Transformer Circuits Thread, 2023.https://transformer-circuits.pub/2023/monosemantic-features/index.html.
Bubeck et al. (2023)
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.URL https://arxiv.org/abs/2303.12712.
Chalnev et al. (2024)
Sviatoslav Chalnev, Matthew Siu, and Arthur Conmy.Improving Steering Vectors by Targeting Sparse Autoencoder Features, 2024.URL http://arxiv.org/abs/2411.02193.
Chanin et al. (2024)
David Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, and Joseph Bloom.A is for absorption: Studying feature splitting and absorption in sparse autoencoders, 2024.URL https://arxiv.org/abs/2409.14507.
Chughtai et al. (2023)
Bilal Chughtai, Lawrence Chan, and Neel Nanda.A toy model of universality: reverse engineering how networks learn group operations.In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023.
Cunningham et al. (2023)
Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey.Sparse autoencoders find highly interpretable features in language models, 2023.URL https://arxiv.org/abs/2309.08600.
Denison et al. (2024)
Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez, and Evan Hubinger.Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models, June 2024.URL http://arxiv.org/abs/2406.10162.
Ding et al. (2023)
Zhiwei Ding, Dat T. Tran, Kayla Ponder, Erick Cobos, Zhuokun Ding, Paul G. Fahey, Eric Wang, Taliah Muhammad, Jiakun Fu, Santiago A. Cadena, Stelios Papadopoulos, Saumil Patel, Katrin Franke, Jacob Reimer, Fabian H. Sinz, Alexander S. Ecker, Xaq Pitkow, and Andreas S. Tolias.Bipartite invariance in mouse primary visual cortex.bioRxiv, March 2023.doi: 10.1101/2023.03.15.532836.URL https://www.biorxiv.org/content/10.1101/2023.03.15.532836v1.Preprint.
Eldan (2023)
Ronen Eldan.Tinystories-1layer-21m.https://huggingface.co/roneneldan/TinyStories-1Layer-21M, 2023.
EleutherAI (2023)
EleutherAI.Sparse autoencoders (sae) repository.https://github.com/EleutherAI/sae, 2023.
Elhage et al. (2021)
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.A mathematical framework for transformer circuits.Transformer Circuits Thread, 2021.https://transformer-circuits.pub/2021/framework/index.html.
Elhage et al. (2022)
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah.Toy models of superposition.Transformer Circuits Thread, 2022.https://transformer-circuits.pub/2022/toy_model/index.html.
Engels et al. (2024)
Joshua Engels, Logan Riggs, and Max Tegmark.Decomposing the dark matter of sparse autoencoders, 2024.URL https://arxiv.org/abs/2410.14670.
Engels et al. (2025)
Joshua Engels, Eric J. Michaud, Isaac Liao, Wes Gurnee, and Max Tegmark.Not all language model features are one-dimensionally linear, 2025.URL https://arxiv.org/abs/2405.14860.
Foote et al. (2023)
Alex Foote, Neel Nanda, Esben Kran, Ioannis Konstas, Shay Cohen, and Fazl Barez.Neuron to graph: Interpreting language model neurons at scale, 2023.URL https://arxiv.org/abs/2305.19911.
Gao et al. (2025)
Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu.Scaling and evaluating sparse autoencoders.In The Thirteenth International Conference on Learning Representations, 2025.URL https://openreview.net/forum?id=tcsZt9ZNKD.
Garde et al. (2023)
Albert Garde, Esben Kran, and Fazl Barez.Deepdecipher: Accessing and investigating neuron activation in large language models, 2023.URL https://arxiv.org/abs/2310.01870.
Gokaslan and Cohen (2019)
Aaron Gokaslan and Vanya Cohen.Openwebtext corpus.http://Skylion007.github.io/OpenWebTextCorpus, 2019.Accessed: March 12 2025.
Goldstein et al. (2024)
Ariel Goldstein, Avigail Grinstein-Dabush, Mariano Schain, Haocheng Wang, Zhuoqiao Hong, Bobbi Aubrey, Samuel A. Nastase, Zaid Zada, Eric Ham, Amir Feder, Harshvardhan Gazula, Eliav Buchnik, Werner Doyle, Sasha Devore, Patricia Dugan, Roi Reichart, Daniel Friedman, Michael Brenner, Avinatan Hassidim, Orrin Devinsky, Adeen Flinker, and Uri Hasson.Alignment of brain embeddings and artificial contextual embeddings in natural language points to common geometric patterns.Nature Communications, 15(1):2768, 2024.doi: 10.1038/s41467-024-46631-y.
Greenblatt et al. (2024)
Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax, Jack Chen, David Duvenaud, Akbir Khan, Julian Michael, Sören Mindermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, and Evan Hubinger.Alignment faking in large language models, 2024.URL http://arxiv.org/abs/2412.14093.arXiv:2412.14093 [cs].
Gurnee et al. (2024)
Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah, Qinyi Sun, Will Hathaway, Neel Nanda, and Dimitris Bertsimas.Universal neurons in gpt2 language models, 2024.URL https://arxiv.org/abs/2401.12181.
Heap et al. (2025)
Thomas Heap, Tim Lawson, Lucy Farnik, and Laurence Aitchison.Sparse autoencoders can interpret randomly initialized transformers, 2025.URL https://arxiv.org/abs/2501.17727.
Hendrycks et al. (2023)
Dan Hendrycks, Mantas Mazeika, and Thomas Woodside.An overview of catastrophic ai risks, 2023.URL https://arxiv.org/abs/2306.12001.
Hindupur et al. (2025)
Sai Sumedh R. Hindupur, Ekdeep Singh Lubana, Thomas Fel, and Demba Ba.Projecting assumptions: The duality between sparse autoencoders and concept geometry, 2025.URL https://arxiv.org/abs/2503.01822.
Hotelling (1936)
Harold Hotelling.Relations between two sets of variates.Biometrika, 28(3/4):321–377, 1936.
Huh et al. (2024)
Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola.The platonic representation hypothesis, 2024.
Karvonen et al. (2024)
A. Karvonen, C. Rager, J. Lin, C. Tigges, J. Bloom, D. Chanin, Y.-T. Lau, E. Farrell, A. Conmy, C. McDougall, K. Ayonrinde, M. Wearden, S. Marks, and N. Nanda.Saebench: A comprehensive benchmark for sparse autoencoders, 2024.URL https://www.neuronpedia.org/sae-bench/info.Accessed: March 12, 2025.
Kissane et al. (2024a)
Connor Kissane, Robert Krzyzanowski, Arthur Conmy, and Neel Nanda.SAEs (usually) Transfer Between Base and Chat Models, 2024a.URL https://www.lesswrong.com/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models.Accessed: March 15, 2025.
Kissane et al. (2024b)
Connor Kissane, robertzk, Neel Nanda, and Arthur Conmy.Saes are highly dataset dependent: a case study on the refusal direction, November 2024b.URL https://www.lesswrong.com/posts/rtp6n7Z23uJpEH7od/saes-are-highly-dataset-dependent-a-case-study-on-the.LessWrong post; accessed March 12, 2025.
Klabunde et al. (2023)
Max Klabunde, Tobias Schumacher, Markus Strohmaier, and Florian Lemmerich.Similarity of neural network models: A survey of functional and representational measures, 2023.URL https://arxiv.org/abs/2305.06329.
Klabunde et al. (2024)
Max Klabunde, Tassilo Wald, Tobias Schumacher, Klaus Maier-Hein, Markus Strohmaier, and Florian Lemmerich.Resi: A comprehensive benchmark for representational similarity measures, 2024.URL https://arxiv.org/abs/2408.00531.
Kondapaneni et al. (2025)
Neehar Kondapaneni, Oisin Mac Aodha, and Pietro Perona.Representational similarity via interpretable visual concepts.In The Thirteenth International Conference on Learning Representations, 2025.URL https://openreview.net/forum?id=ih3BJmIZbC.
Kornblith et al. (2019)
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.Similarity of neural network representations revisited, 2019.URL https://arxiv.org/abs/1905.00414.
Kriegeskorte et al. (2008)
Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini.Representational similarity analysis – connecting the branches of systems neuroscience.Frontiers in Systems Neuroscience, 2:4, 2008.doi: 10.3389/neuro.06.004.2008.URL https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full.
Kutsyk et al. (2024)
Taras Kutsyk, Tommaso Mencattini, and Ciprian Florea.Do sparse autoencoders (saes) transfer across base and finetuned language models?, sep 2024.URL https://www.lesswrong.com/posts/bsXPTiAhhwt5nwBW3/do-sparse-autoencoders-saes-transfer-across-base-and.LessWrong post; accessed March 12, 2025.
Leask et al. (2024)
Patrick Leask, Bart Bussmann, and Neel Nanda.Calendar Feature Geometry in GPT-2 Layer 8 Residual Stream SAEs.LessWrong, August 2024.URL https://www.lesswrong.com/posts/WsPyunwpXYCM2iN6t/calendar-feature-geometry-in-gpt-2-layer-8-residual-stream.Mirror on AI Alignment Forum; accessed May 18, 2025.
Leask et al. (2025)
Patrick Leask, Bart Bussmann, Michael T. Pearce, Joseph I. Bloom, Curt Tigges, N. Al Moubayed, Lee Sharkey, and Neel Nanda.Sparse autoencoders do not find canonical units of analysis.In Proceedings of the 13th International Conference on Learning Representations (ICLR 2025), 2025.URL https://openreview.net/forum?id=9ca9eHNrdH.
Li et al. (2025)
Yuxiao Li, Eric J. Michaud, David D. Baek, Joshua Engels, Xiaoqing Sun, and Max Tegmark.The geometry of concepts: Sparse autoencoder feature structure.Entropy, 27(4):344, March 2025.ISSN 1099-4300.doi: 10.3390/e27040344.URL http://dx.doi.org/10.3390/e27040344.
Lieberum et al. (2024)
Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, János Kramár, Anca Dragan, Rohin Shah, and Neel Nanda.Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2, 2024.URL https://arxiv.org/abs/2408.05147.
Lindsey et al. (2024)
Jack Lindsey, Adly Templeton, Jonathan Marcus, Thomas Conerly, Joshua Batson, and Christopher Olah.Sparse crosscoders for cross-layer features and model diffing.https://transformer-circuits.pub/2024/crosscoders/index.html, January 2024.Transformer Circuits Thread.
Makhzani and Frey (2013)
Alireza Makhzani and Brendan Frey.k-sparse autoencoders.arXiv preprint arXiv:1312.5663, 2013.
Merullo et al. (2023)
Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick.Linearly mapping from image to text space, 2023.URL https://arxiv.org/abs/2209.15162.
Mini et al. (2023)
Ulisse Mini, Peli Grietzer, Mrinank Sharma, Austin Meek, Monte MacDiarmid, and Alexander Matt Turner.Understanding and Controlling a Maze-Solving Policy Network, 2023.URL http://arxiv.org/abs/2310.08043.arXiv:2310.08043 [cs].
Mishra-Sharma et al. (2025)
Siddharth Mishra-Sharma, Trenton Bricken, Jack Lindsey, Adam Jermyn, Jonathan Marcus, Kelley Rivoire, Christopher Olah, and Thomas Henighan.Insights on crosscoder model diffing.https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html, February 2025.Transformer Circuits Thread.
Naveed et al. (2024)
Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian.A comprehensive overview of large language models, 2024.URL https://arxiv.org/abs/2307.06435.
Neo et al. (2024)
Clement Neo, Shay B. Cohen, and Fazl Barez.Interpreting context look-ups in transformers: Investigating attention-mlp interactions, 2024.URL https://arxiv.org/abs/2402.15055.
Ngo et al. (2022)
Richard Ngo, Lawrence Chan, and Sören Mindermann.The Alignment Problem from a Deep Learning Perspective, 2022.URL http://arxiv.org/abs/2209.00626.arXiv:2209.00626 [cs].
O’Brien et al. (2024)
Kyle O’Brien, David Majercak, Xavier Fernandes, Richard Edgar, Jingya Chen, Harsha Nori, Dean Carignan, Eric Horvitz, and Forough Poursabzi-Sangde.Steering language model refusal with sparse autoencoders, 2024.URL https://arxiv.org/abs/2411.11296.
Olah and Batson (2023)
Chris Olah and Josh Batson.Feature manifold toy model.Transformer Circuits May Update, May 2023.URL https://transformer-circuits.pub/2023/may-update/index.html#feature-manifolds.
Olah et al. (2020)
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.Zoom in: An introduction to circuits.Distill, 2020.doi: 10.23915/distill.00024.001.https://distill.pub/2020/circuits/zoom-in.
Olshausen and Field (1997)
Bruno A. Olshausen and David J. Field.Sparse coding with an overcomplete basis set: A strategy employed by v1?Vision Research, 37(23):3311–3325, 1997.ISSN 0042-6989.doi: https://doi.org/10.1016/S0042-6989(97)00169-7.URL https://www.sciencedirect.com/science/article/pii/S0042698997001697.
Park et al. (2024a)
Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch.The geometry of categorical and hierarchical concepts in large language models, 2024a.
Park et al. (2024b)
Kiho Park, Yo Joong Choe, and Victor Veitch.The linear representation hypothesis and the geometry of large language models.In Proceedings of the 41st International Conference on Machine Learning, ICML’24. JMLR.org, 2024b.
Paulo and Belrose (2025)
Gonçalo Paulo and Nora Belrose.Sparse autoencoders trained on the same data learn different features, 2025.URL https://arxiv.org/abs/2501.16615.
Raghu et al. (2017)
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Narain Sohl-Dickstein.Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability.In Neural Information Processing Systems, 2017.URL https://api.semanticscholar.org/CorpusID:23890457.
Rajamanoharan et al. (2024a)
Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, János Kramár, Rohin Shah, and Neel Nanda.Improving dictionary learning with gated sparse autoencoders, 2024a.URL https://arxiv.org/abs/2404.16014.
Rajamanoharan et al. (2024b)
Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, János Kramár, and Neel Nanda.Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders, 2024b.URL https://arxiv.org/abs/2407.14435.
Rimsky et al. (2024)
Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner.Steering llama 2 via contrastive activation addition, 2024.
Schubert et al. (2021)
Ludwig Schubert, Chelsea Voss, Nick Cammarata, Gabriel Goh, and Chris Olah.High-low frequency detectors.Distill, 2021.doi: 10.23915/distill.00024.005.https://distill.pub/2020/circuits/frequency-edges.
Shah et al. (2022)
Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac Kenton.Goal Misgeneralization: Why Correct Specifications Aren’t Enough For Correct Goals, November 2022.URL http://arxiv.org/abs/2210.01790.
Sharkey et al. (2022)
Lee Sharkey, Dan Braun, and Beren Millidge.Interim research report: Taking features out of superposition with sparse autoencoders.AI Alignment Forum, 2022.URL https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition.
Sharkey et al. (2024)
Lee Sharkey, Lucius Bushnaq, Dan Braun, Stefan Hex, and Nicholas Goldowsky-Dill.A list of 45 mech interp project ideas from apollo research.https://www.lesswrong.com/posts/KfkpgXdgRheSRWDy8/a-list-of-45-mech-interp-project-ideas-from-apollo-research, 2024.Accessed: July 25, 2024.
Sucholutsky et al. (2023)
Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love, Erin Grant, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nori Jacoby, Qiuyi Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. O’Connell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert Müller, Mariya Toneva, and Thomas L. Griffiths.Getting aligned on representational alignment.CoRR, abs/2310.13018, 2023.
Team et al. (2024a)
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy.Gemma: Open models based on gemini research and technology, 2024a.URL https://arxiv.org/abs/2403.08295.
Team et al. (2024b)
Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev.Gemma 2: Improving open language models at a practical size, 2024b.URL https://arxiv.org/abs/2408.00118.
team et al. (2024)
LCM team, Loïc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-jussà, David Dale, Hady Elsahar, Kevin Heffernan, João Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, and Holger Schwenk.Large concept models: Language modeling in a sentence representation space, 2024.URL https://arxiv.org/abs/2412.08821.
Templeton et al. (2024)
Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan.Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet.Transformer Circuits Thread, 2024.URL https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html.
Thasarathan et al. (2025)
Harrish Thasarathan, Julian Forsyth, Thomas Fel, Matthew Kowal, and Konstantinos Derpanis.Universal sparse autoencoders: Interpretable cross-model concept alignment, 2025.URL https://arxiv.org/abs/2502.03714.
Till (2023)
Demian Till.Do sparse autoencoders find ’true features’?, 2023.URL https://www.lesswrong.com/posts/QoR8noAB3Mp2KBA4B/do-sparse-autoencoders-find-true-features.Accessed: January 29, 2025.
Turner et al. (2024)
Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Monte MacDiarmid.Steering language models with activation engineering, 2024.URL https://arxiv.org/abs/2308.10248.
Vaswani et al. (2017)
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.Attention is all you need.Advances in neural information processing systems, 30, 2017.
Wang et al. (2018a)
Liwei Wang, Lunjia Hu, Jiayuan Gu, Yue Wu, Zhiqiang Hu, Kun He, and John Hopcroft.Towards understanding learning representations: To what extent do different neural networks learn the same representation.In Advances in Neural Information Processing Systems, volume 31, 2018a.URL https://proceedings.neurips.cc/paper/2018/file/5fc34ed307aac159a30d81181c99847e-Paper.pdf.
Wang et al. (2018b)
Liwei Wang, Lunjia Hu, Jiayuan Gu, Yue Wu, Zhiqiang Hu, Kun He, and John Hopcroft.Towards understanding learning representations: To what extent do different neural networks learn the same representation.In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, pages 9607–9616, Red Hook, NY, USA, 2018b. Curran Associates Inc.ISBN 9781510860964.
Weber et al. (2024)
Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang.Redpajama: an open dataset for training large language models, 2024.URL https://arxiv.org/abs/2411.12372.
Ye et al. (2024)
Christine Ye, Charles O’Neill, John F Wu, and Kartheik G. Iyer.Sparse autoencoders for dense text embeddings reveal hierarchical feature sub-structure.In NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning, 2024.URL https://openreview.net/forum?id=hjROYyfxfO.
Yosinski et al. (2014)
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.How transferable are features in deep neural networks?In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS’14, page 3320–3328, Cambridge, MA, USA, 2014. MIT Press.
Zou et al. (2023)
Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks.Representation engineering: A top-down approach to ai transparency, 2023
